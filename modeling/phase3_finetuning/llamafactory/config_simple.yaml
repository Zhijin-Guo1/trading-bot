# Simple Configuration for Qwen2.5-1.5B-Instruct
# Minimal config to avoid compatibility issues

### Model ###
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
trust_remote_code: true

### Method ###
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### LoRA ###
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.1

### Data ###
dataset_dir: data
dataset: 8k_train
template: qwen
cutoff_len: 2048
preprocessing_num_workers: 4
overwrite_cache: true

### Training ###
output_dir: ./outputs/qwen2.5-1.5b-lora
overwrite_output_dir: true
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
num_train_epochs: 3
learning_rate: 5e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
logging_steps: 10
save_steps: 100
save_total_limit: 3
report_to: none
logging_dir: ./logs
optim: adamw_torch